```python Python
from supadata import Supadata

supadata = Supadata("YOUR_API_KEY")

# Get crawl results
# This automatically handles pagination and returns all pages
crawl_result = supadata.web.get_crawl_results(job_id=job_id)

if crawl_result.status == "completed":
    print("Crawl job completed successfully!")
    print(f"Total pages crawled: {len(crawl_result.pages)}")
    
    # Process each page
    for i, page in enumerate(crawl_result.pages):
        print(f"Page {i + 1}: {page.name}")
        print(f"URL: {page.url}")
        print(f"Description: {page.description}")
        print(f"Content preview: {page.content[:100]}...")
        print("---")
elif crawl_result.status == "failed":
    print(f"Crawl job failed: {crawl_result.error}")
else:
    print(f"Job status: {crawl_result.status}")
```